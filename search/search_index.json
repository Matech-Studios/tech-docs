{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Playbook: Recipes Recommender for StarvApp \u00b6 Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done. Contents \u00b6 These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution ( 3 min read ) Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. ( 3 min read ) Chapter 2: Preparation : Setup and preparation of the resources and the working environment. ( 4 min read ) Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. ( 10 min read ) Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. Epilogue : Facts occurred after the solution was deployed and validated (optional). How-to Guides \u00b6 These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-08-28 hello@matechstudios.com","title":"Home"},{"location":"#playbook-recipes-recommender-for-starvapp","text":"Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done.","title":"Playbook: Recipes Recommender for StarvApp"},{"location":"#contents","text":"These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution ( 3 min read ) Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. ( 3 min read ) Chapter 2: Preparation : Setup and preparation of the resources and the working environment. ( 4 min read ) Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. ( 10 min read ) Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. Epilogue : Facts occurred after the solution was deployed and validated (optional).","title":"Contents"},{"location":"#how-to-guides","text":"These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-08-28 hello@matechstudios.com","title":"How-to Guides"},{"location":"0_preface/","text":"Preface \u00b6 Abstract This section introduces the most relevant information of the solution detailed in the playbook. ( 3 min read ) Background \u00b6 StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. In order to make suitable recommendations, we decided to create a new service that interact with the existing services in the application to provide a selection of recommended recipes for the input ingredients entered by the user. The approach taken to recommend recipes is through a basic schema that uses the matching ingredients and their frecuencies in the database to rank the best recipes for a given input. Then, a Python service was developed that connects to a AMQP queue to recipe input and deliver recommendations in real time. This playbook is intented to explain how this recipes recommender was developed, from the understanding of the problem to the validation of the solution integrated in the application. It is written for a reader that is interested in knowing how to obtain a recommender system for this context of the application, so the information will have technical details at different levels. Goal \u00b6 Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database. Scope \u00b6 The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version. Technologies \u00b6 These are the most relevant technologies used during the development. Languages \u00b6 Name Version Description Python 3.8 Main development Bash 5.0 Util scripts Data Processing \u00b6 Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring FastText 0.9 Classification of ingredients from text descriptions Messages broker \u00b6 Name Version Description Aio-pika 6.7 Connection to RabbitMQ (AMQP connection) with asyncio capabilities Testing \u00b6 Name Version Description Pytest 6.2 Unit and System testing of service Team \u00b6 Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Preface"},{"location":"0_preface/#preface","text":"Abstract This section introduces the most relevant information of the solution detailed in the playbook. ( 3 min read )","title":"Preface"},{"location":"0_preface/#background","text":"StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. In order to make suitable recommendations, we decided to create a new service that interact with the existing services in the application to provide a selection of recommended recipes for the input ingredients entered by the user. The approach taken to recommend recipes is through a basic schema that uses the matching ingredients and their frecuencies in the database to rank the best recipes for a given input. Then, a Python service was developed that connects to a AMQP queue to recipe input and deliver recommendations in real time. This playbook is intented to explain how this recipes recommender was developed, from the understanding of the problem to the validation of the solution integrated in the application. It is written for a reader that is interested in knowing how to obtain a recommender system for this context of the application, so the information will have technical details at different levels.","title":"Background"},{"location":"0_preface/#goal","text":"Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database.","title":"Goal"},{"location":"0_preface/#scope","text":"The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version.","title":"Scope"},{"location":"0_preface/#technologies","text":"These are the most relevant technologies used during the development.","title":"Technologies"},{"location":"0_preface/#languages","text":"Name Version Description Python 3.8 Main development Bash 5.0 Util scripts","title":"Languages"},{"location":"0_preface/#data-processing","text":"Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring FastText 0.9 Classification of ingredients from text descriptions","title":"Data Processing"},{"location":"0_preface/#messages-broker","text":"Name Version Description Aio-pika 6.7 Connection to RabbitMQ (AMQP connection) with asyncio capabilities","title":"Messages broker"},{"location":"0_preface/#testing","text":"Name Version Description Pytest 6.2 Unit and System testing of service","title":"Testing"},{"location":"0_preface/#team","text":"Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Team"},{"location":"1_understanding/","text":"Chapter 1: Understanding \u00b6 Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 3 min read ) Context \u00b6 StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information. Info Data source of recipes was changed during the project due to quality issued found. See Chapter 3 for more details. The stakeholders involved in this development are mainly the Product Owner of the application and the teams that develop the other services: Frontend team , in charge of the UI and the UX of the application. Backend team , in charge of the services of the application (endpoints, database, integrations). Analytics team (which develops this service), in charge of extracting data analytics to enrich the information presented to the user. DevOps team , in charge of the infrastructure where the application is deployed. Finally, the application will be in Spanish language to allow a deployment in Argentina. Therefore, the text information to process will be in that language. Problem \u00b6 Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project's team if possible, to ensure there are opinions biased by the project flow. Definitions \u00b6 Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Chapter 1: Understanding"},{"location":"1_understanding/#chapter-1-understanding","text":"Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 3 min read )","title":"Chapter 1: Understanding"},{"location":"1_understanding/#context","text":"StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information. Info Data source of recipes was changed during the project due to quality issued found. See Chapter 3 for more details. The stakeholders involved in this development are mainly the Product Owner of the application and the teams that develop the other services: Frontend team , in charge of the UI and the UX of the application. Backend team , in charge of the services of the application (endpoints, database, integrations). Analytics team (which develops this service), in charge of extracting data analytics to enrich the information presented to the user. DevOps team , in charge of the infrastructure where the application is deployed. Finally, the application will be in Spanish language to allow a deployment in Argentina. Therefore, the text information to process will be in that language.","title":"Context"},{"location":"1_understanding/#problem","text":"Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project's team if possible, to ensure there are opinions biased by the project flow.","title":"Problem"},{"location":"1_understanding/#definitions","text":"Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Definitions"},{"location":"2_preparation/","text":"Chapter 2: Preparation \u00b6 Abstract This chapter covers the preparation in terms of the architecture defined for the solution, as well as the setup of the development environment. ( 4 min read ) Architecture \u00b6 The solution must be integrated with the rest of the application by interacting with the backend through a proper bus. Therefore, this is the general architecture that the service had to meet to allow the integration. These are the steps that the application takes in the normal flow: The App registers the input of the user and requests recipes to the Recipes API . The Recipes API gets data from the Database and do the pre-processing of the data to prepare it for the Data Analytics service. The Recipes API publishes the pre-processed data to the Events Bus and waits for the recommended recipes that the Data Analitycs will return by subscribing to a configured topic of the bus (e.g. recipes/{clientId} ). The Data Analitycs is subscribed to the configured topic of the Event Bus and gets the pre-processed data published by the Recipes API . The Data Analitycs processes the data to get a set of recommended recipes, and publishes the selected recipes to the Event Bus on the same configured topic. The Recipes API gets the data with the recommended recipes from the Events Bus . The Recipes API returns the response to the App . It is important to highlight that if this process from step 4 to 6 takes more than a configured window in seconds, the Recipes API will return a default response to the App which is just a header of the recipes retrieved in the step 2. Setup \u00b6 Code Repository \u00b6 At this moment of the project, we didn't have a proper scaffolding nor boilerplate to setup the code repository. Therefore, a new repository was started from scratch, where the following aspects were solved to start the development: Repository name following the name convention followed in Matech, so it was named Matech.Starvapp.RecipesRecomender.Analytics . A Python package to develop the actual logic to process the data A folder with the script for the service A folder to place the notebooks intended for exploration and checks A folder to develop the tests for the solution at unit and system level A folder for development tools (e.g. run linter, set environment, run tests) README.md file with relevant information Environment management through virtualenv Testing handled with pytest The repository was published here . You can check how the initial structure was defined in the Pull Request #1 , obtaining this: \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 starvapprecom <- Core package of this project. \u2502 \u251c\u2500\u2500 approaches <- Module to develop approaches for recipes recommendations \u2502 \u251c\u2500\u2500 scrapers <- Module to handle scrapers of recipes \u2502 \u2514\u2500\u2500 utils <- Module to provide utils in general \u2502 \u251c\u2500\u2500 data <- Folder for datasets to be used locally \u2502 \u251c\u2500\u2500 docs <- Project documentation and resources \u2502 \u251c\u2500\u2500 notebooks <- Place to store all Jupyter notebooks \u2502 \u251c\u2500\u2500 scripts <- Scripts to execute services and other functions \u2502 \u251c\u2500\u2500 tests <- Unit and System tests of the core library \u2502 \u251c\u2500\u2500 tools <- Tools for the development of this project \u2502 \u2514\u2500\u2500\u2500 requirements.txt <- File that specifies the dependencies for this project Environment \u00b6 All the services in StarvApp are deployed through Kubernetes , so each of them must provide a Dockerfile that allows the DevOps team to achieve the actual deployment. Therefore, for this repository we provided: A Dockerfile that creates and serves the recommender solution A Dockerfile_rabbitmq that creates a RabbitMQ server for testing purposes A docker-compose.yml to configure the connection between these two previous services, for testing purposes. In addition, to allow an easy way to make local development, there are tools to create and use a virtualenv which is easier to handle for local tests. Given that, the development evolved from the explained \"boilerplate\" to then create the PR #2 , where the following aspects were solved: File Dockerfile to build and serve the application File docker-compose.yml to test quickly a deployment before delivering the application Service developed to connect to a RabbitMQ queue and serve recipes recommendations After that, the development proceed to enhance and fix some aspects, but in the PR #7 the repository evolved to have these aspects done: Github Actions to keep an internal Continuous Integration Azure Pipelines connection to serve this repo in the flow of Continuous Integration of the entire StarvApp Testing to ensure code coverage during the CI workflows You can check these configurations in the actual code repository.","title":"Chapter 2: Preparation"},{"location":"2_preparation/#chapter-2-preparation","text":"Abstract This chapter covers the preparation in terms of the architecture defined for the solution, as well as the setup of the development environment. ( 4 min read )","title":"Chapter 2: Preparation"},{"location":"2_preparation/#architecture","text":"The solution must be integrated with the rest of the application by interacting with the backend through a proper bus. Therefore, this is the general architecture that the service had to meet to allow the integration. These are the steps that the application takes in the normal flow: The App registers the input of the user and requests recipes to the Recipes API . The Recipes API gets data from the Database and do the pre-processing of the data to prepare it for the Data Analytics service. The Recipes API publishes the pre-processed data to the Events Bus and waits for the recommended recipes that the Data Analitycs will return by subscribing to a configured topic of the bus (e.g. recipes/{clientId} ). The Data Analitycs is subscribed to the configured topic of the Event Bus and gets the pre-processed data published by the Recipes API . The Data Analitycs processes the data to get a set of recommended recipes, and publishes the selected recipes to the Event Bus on the same configured topic. The Recipes API gets the data with the recommended recipes from the Events Bus . The Recipes API returns the response to the App . It is important to highlight that if this process from step 4 to 6 takes more than a configured window in seconds, the Recipes API will return a default response to the App which is just a header of the recipes retrieved in the step 2.","title":"Architecture"},{"location":"2_preparation/#setup","text":"","title":"Setup"},{"location":"2_preparation/#code-repository","text":"At this moment of the project, we didn't have a proper scaffolding nor boilerplate to setup the code repository. Therefore, a new repository was started from scratch, where the following aspects were solved to start the development: Repository name following the name convention followed in Matech, so it was named Matech.Starvapp.RecipesRecomender.Analytics . A Python package to develop the actual logic to process the data A folder with the script for the service A folder to place the notebooks intended for exploration and checks A folder to develop the tests for the solution at unit and system level A folder for development tools (e.g. run linter, set environment, run tests) README.md file with relevant information Environment management through virtualenv Testing handled with pytest The repository was published here . You can check how the initial structure was defined in the Pull Request #1 , obtaining this: \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 starvapprecom <- Core package of this project. \u2502 \u251c\u2500\u2500 approaches <- Module to develop approaches for recipes recommendations \u2502 \u251c\u2500\u2500 scrapers <- Module to handle scrapers of recipes \u2502 \u2514\u2500\u2500 utils <- Module to provide utils in general \u2502 \u251c\u2500\u2500 data <- Folder for datasets to be used locally \u2502 \u251c\u2500\u2500 docs <- Project documentation and resources \u2502 \u251c\u2500\u2500 notebooks <- Place to store all Jupyter notebooks \u2502 \u251c\u2500\u2500 scripts <- Scripts to execute services and other functions \u2502 \u251c\u2500\u2500 tests <- Unit and System tests of the core library \u2502 \u251c\u2500\u2500 tools <- Tools for the development of this project \u2502 \u2514\u2500\u2500\u2500 requirements.txt <- File that specifies the dependencies for this project","title":"Code Repository"},{"location":"2_preparation/#environment","text":"All the services in StarvApp are deployed through Kubernetes , so each of them must provide a Dockerfile that allows the DevOps team to achieve the actual deployment. Therefore, for this repository we provided: A Dockerfile that creates and serves the recommender solution A Dockerfile_rabbitmq that creates a RabbitMQ server for testing purposes A docker-compose.yml to configure the connection between these two previous services, for testing purposes. In addition, to allow an easy way to make local development, there are tools to create and use a virtualenv which is easier to handle for local tests. Given that, the development evolved from the explained \"boilerplate\" to then create the PR #2 , where the following aspects were solved: File Dockerfile to build and serve the application File docker-compose.yml to test quickly a deployment before delivering the application Service developed to connect to a RabbitMQ queue and serve recipes recommendations After that, the development proceed to enhance and fix some aspects, but in the PR #7 the repository evolved to have these aspects done: Github Actions to keep an internal Continuous Integration Azure Pipelines connection to serve this repo in the flow of Continuous Integration of the entire StarvApp Testing to ensure code coverage during the CI workflows You can check these configurations in the actual code repository.","title":"Environment"},{"location":"3_execution/","text":"Chapter 3: Execution \u00b6 Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( 10 min read ) Initial analysis \u00b6 Once the repository was prepared, and the initial requirements were defined, we should start from the analysis of the available data to check the following aspects: - Available information to be used for the recommender schema - Quality of data, and corrections to be done - Pre-processing steps required to use the data The first data source implemented was Spoonacular through a paid subscription to RapidAPI . Since the involved recipes were in English, a translator service was implemented as well to have Spanish data. In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, a Data Quality Control was conducted. Check the How-to: Data Quality Control document for details. Through this notebook , the analysis was done where in general aspects the data observed was fine. However, at that point and in later iterations there were data issues found like: - Thousands of ingredients, making hard to achieve proper matching rules for the recommender (check these issues identified ) - Bad translations of the implemented service (also noticed in the previous issues identified) - Missing information in some fields, especially for instructions of recipes Therefore, the data sources was marked as not reliable , and an extra work was required to identify a new source and scrape recipes data from it. New data source \u00b6 After reviewing several potential data sources (having fields required by the StarvApp, and without legal restrictions), a new one was found. Therefore, a scraper script was developed to get datasets from there with some defined configuration . Here some important limitations to know about the scraper: - Execution must be done manually, with a proper update of the YAML configuration mentioned - Paths to resulting datasets must be updated in the code as well The resulting datasets in CSV format were controlled in a new data quality check conducted in this notebook , and then shared to the Backend team to a proper upload to the Database of the StarvApp. In addition, the snapshots of datasets obtained were stored in One Drive for availability. Info This data extraction process was part of a workaround task handled after finding data issues in Spoonacular. Later work about automation of data extraction from these sources will be required in further steps. It is important to mention that ingredients in recipes extracted from this source were not in a normalized basis as in Spoonacular, but they were in format of free text description that required to be processed and classified as described below. Ingredients classification \u00b6 Given a free text description of an ingredient, the challenge is to give it a normalized name to make easier the retrieval of recipes for a query of entered ingredients. For example, if you have an ingredient with description \"2 cucharadas de aceite de oliva\" you expect to classify it as \"Aceite de oliva\". However, this is not an easy task due to the given aspects that may happen on the text: Information about quantity, size and units that may variate for each kind of recipe (e.g. \"1 kg de aj\u00edes morrones rojos grandes\") Information about how the ingredient should be prepared (e.g. \"1 cebolla mediana picada fina\") Personal comments about the quality of the ingredient (e.g. \"2 hojas de laurel fileteadas (mejor fresco)\") Some ingredients share very similar names (e.g. \"Cebolla\" & \"Cebolla de verdeo\", \"Aceite\" & \"Aceite de oliva\") Some ingredients even change of name depending on the region (e.g. \"Calabacin\" means \"Zucchini\" in other countries) Therefore, the approach taken was to train a classifier to process a text description and tell which is the corresponding ingredient. To tackle the given complexity in the text, a FastText model was defined due to be easy to adjust but powerful as it is a neural network designed for this purpose. The first step was to prepare a labeled dataset to train and validate the model with 2 fields per record (a text description, and a \"label\" indicating the expected ingredient classification), which was built as it follows: Label some few records (e.g. 5 per class) by manually indicating which ingredient to put for a given text description Remove those records for ingredients very specific and hard to find in usual recipes. This means that only recipes with supported ingredients will be valid in the database . Train a model and use it to classify new recors (e.g. 10 per class) Correct some mislabeled records by manually and loop until you have a bigger labeled dataset (e.g. 100 records per class) A labeled dataset was obtained and stored in One Drive, so then it was used to train a FastText model. A script was developed to do that, which does the following steps - Load labeled dataset - Pre-process the text descriptions (e.g. apply lowercase, remove special characters and numbers, remove words from a blacklist) - Split train-test sets for cross-validation - Train model with some defined parameters - Get results about train and test sets to validate the performance of the model Once the model was obtained, then it is ready to be used to classify the extracted recipes to build the database. As a result, we can have the datasets with proper fields and ingredients ready to be used for the application. Recommender approach \u00b6 Given an input from the user with ingredients, and a database with recipes, the idea was to have a recommender system to decide the top of recipes to return with some criteria. In this project, there were two approaches explored in this order: Basic scores Weight ingredients Both of them were intended for a \"cold start\" , where no users history is available. Therefore, recommendations are obtained only based on the recipes information, not from users. Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it. Approach 1: Basic Scores \u00b6 Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach Idea \u00b6 In this \"naive\" approach, the idea is to get the recipes with the best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach. Metrics \u00b6 In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 The problem of this approach is that it does not take into account some concept of relevancy on ingredients involved in a recommended recipe. So, as a result, some \"not common\" recipes may be selected when having few matches of ingredients for a given input, and that was a not desired aspect that was analyzed to propose a new solution. More details about approach in this Jupyter notebook . Approach 2: Weight Ingredients \u00b6 To tackle the problem of recommendations with not common ingredients when there are few matched ingredients, a new approach was analyzed to \"weight\" ingredients to achieve a selection of \"feasible\" recipes on the recommendations delivered. Idea \u00b6 Given that context, we could compute some \"weights\" for the given ingredients on the database, where the idea is to express in a score with range [0, 1] how \"common\" is an ingredient. Therefore, the recommendations that will select additional ingredients that are not part of the input, could prefer \"common\" ingredients to be recommended instead of \"not common\" ones. So, through an analysis on the data available, ingredients were processed to get frecuency and scale it into a range of [0,1] with a logarithmic scale. The result was saved into a YAML file , and the approach implemented was simply recommend recipes that: - Are prone to contain the input ingredients (with high score) - Are prone to contain \"common\" ingredients (with the explained score) Info Notice that the YAML file indicates which is the list of supported ingredients, and the relevance they have in the given database. Finally, this was the approach selected to go production . Service development \u00b6 Once the recommendation approach was defined and developed, the final step is to develop and deploy a service that receives requests from the StarvApp with input of users and returns recipes recommendations. Therefore, a script was developed with asyncio to do the following steps: Set an asynchronous connection to an existing RabbitMQ queue with aio-pika When a message is received, the service calls the function to process the input ingredients and return the recommended recipes A response is delivered with the recommendations, on the same RabbitMQ queue. For more details about the integration between services in the application, check the architecture shown in Chapter 2 . To test the service before the deployment, another script was developed to emulate requests with a random input to ensure the service will respond correctly. To deploy this service, a Dockerfile was developed to install the system and run the service as entrypoint. At the same time, another Dockerfile was defined to also run the RabbitMQ service for the application. Notice that both services are managed by the DevOps team in their given infrastructure. Testing \u00b6 For this solution, there where two types of testing implemented: Unit testing: To validate each module or block of code on the starvapprecom package mostly for code coverage purposes, which are implemented as simple functions executed by pytest . System testing: To validate some functional tests , as checks over the recommender with a dataset regarding some specific scenarios , each of them having corresponding checks . Info To run the system tests, a dataset must be available and with a configured valid path Deployment \u00b6 The StarvApp has a Continuous Integration (CI) pipeline configured in terms of Azure pipelines and handled by the DevOps team, so the CI is already managed in this work and the files explained in the Environment section are the contract to ensure correct integration. However, additionally a Github Actions workflow was configured for this repository to make the following checks: Smoke testing based on configured tests Code coverage along the releases Correct linting You can check the configuration and current status in the project's README .","title":"Chapter 3: Execution"},{"location":"3_execution/#chapter-3-execution","text":"Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( 10 min read )","title":"Chapter 3: Execution"},{"location":"3_execution/#initial-analysis","text":"Once the repository was prepared, and the initial requirements were defined, we should start from the analysis of the available data to check the following aspects: - Available information to be used for the recommender schema - Quality of data, and corrections to be done - Pre-processing steps required to use the data The first data source implemented was Spoonacular through a paid subscription to RapidAPI . Since the involved recipes were in English, a translator service was implemented as well to have Spanish data. In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, a Data Quality Control was conducted. Check the How-to: Data Quality Control document for details. Through this notebook , the analysis was done where in general aspects the data observed was fine. However, at that point and in later iterations there were data issues found like: - Thousands of ingredients, making hard to achieve proper matching rules for the recommender (check these issues identified ) - Bad translations of the implemented service (also noticed in the previous issues identified) - Missing information in some fields, especially for instructions of recipes Therefore, the data sources was marked as not reliable , and an extra work was required to identify a new source and scrape recipes data from it.","title":"Initial analysis"},{"location":"3_execution/#new-data-source","text":"After reviewing several potential data sources (having fields required by the StarvApp, and without legal restrictions), a new one was found. Therefore, a scraper script was developed to get datasets from there with some defined configuration . Here some important limitations to know about the scraper: - Execution must be done manually, with a proper update of the YAML configuration mentioned - Paths to resulting datasets must be updated in the code as well The resulting datasets in CSV format were controlled in a new data quality check conducted in this notebook , and then shared to the Backend team to a proper upload to the Database of the StarvApp. In addition, the snapshots of datasets obtained were stored in One Drive for availability. Info This data extraction process was part of a workaround task handled after finding data issues in Spoonacular. Later work about automation of data extraction from these sources will be required in further steps. It is important to mention that ingredients in recipes extracted from this source were not in a normalized basis as in Spoonacular, but they were in format of free text description that required to be processed and classified as described below.","title":"New data source"},{"location":"3_execution/#ingredients-classification","text":"Given a free text description of an ingredient, the challenge is to give it a normalized name to make easier the retrieval of recipes for a query of entered ingredients. For example, if you have an ingredient with description \"2 cucharadas de aceite de oliva\" you expect to classify it as \"Aceite de oliva\". However, this is not an easy task due to the given aspects that may happen on the text: Information about quantity, size and units that may variate for each kind of recipe (e.g. \"1 kg de aj\u00edes morrones rojos grandes\") Information about how the ingredient should be prepared (e.g. \"1 cebolla mediana picada fina\") Personal comments about the quality of the ingredient (e.g. \"2 hojas de laurel fileteadas (mejor fresco)\") Some ingredients share very similar names (e.g. \"Cebolla\" & \"Cebolla de verdeo\", \"Aceite\" & \"Aceite de oliva\") Some ingredients even change of name depending on the region (e.g. \"Calabacin\" means \"Zucchini\" in other countries) Therefore, the approach taken was to train a classifier to process a text description and tell which is the corresponding ingredient. To tackle the given complexity in the text, a FastText model was defined due to be easy to adjust but powerful as it is a neural network designed for this purpose. The first step was to prepare a labeled dataset to train and validate the model with 2 fields per record (a text description, and a \"label\" indicating the expected ingredient classification), which was built as it follows: Label some few records (e.g. 5 per class) by manually indicating which ingredient to put for a given text description Remove those records for ingredients very specific and hard to find in usual recipes. This means that only recipes with supported ingredients will be valid in the database . Train a model and use it to classify new recors (e.g. 10 per class) Correct some mislabeled records by manually and loop until you have a bigger labeled dataset (e.g. 100 records per class) A labeled dataset was obtained and stored in One Drive, so then it was used to train a FastText model. A script was developed to do that, which does the following steps - Load labeled dataset - Pre-process the text descriptions (e.g. apply lowercase, remove special characters and numbers, remove words from a blacklist) - Split train-test sets for cross-validation - Train model with some defined parameters - Get results about train and test sets to validate the performance of the model Once the model was obtained, then it is ready to be used to classify the extracted recipes to build the database. As a result, we can have the datasets with proper fields and ingredients ready to be used for the application.","title":"Ingredients classification"},{"location":"3_execution/#recommender-approach","text":"Given an input from the user with ingredients, and a database with recipes, the idea was to have a recommender system to decide the top of recipes to return with some criteria. In this project, there were two approaches explored in this order: Basic scores Weight ingredients Both of them were intended for a \"cold start\" , where no users history is available. Therefore, recommendations are obtained only based on the recipes information, not from users. Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it.","title":"Recommender approach"},{"location":"3_execution/#approach-1-basic-scores","text":"Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach","title":"Approach 1: Basic Scores"},{"location":"3_execution/#idea","text":"In this \"naive\" approach, the idea is to get the recipes with the best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach.","title":"Idea"},{"location":"3_execution/#metrics","text":"In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 The problem of this approach is that it does not take into account some concept of relevancy on ingredients involved in a recommended recipe. So, as a result, some \"not common\" recipes may be selected when having few matches of ingredients for a given input, and that was a not desired aspect that was analyzed to propose a new solution. More details about approach in this Jupyter notebook .","title":"Metrics"},{"location":"3_execution/#approach-2-weight-ingredients","text":"To tackle the problem of recommendations with not common ingredients when there are few matched ingredients, a new approach was analyzed to \"weight\" ingredients to achieve a selection of \"feasible\" recipes on the recommendations delivered.","title":"Approach 2: Weight Ingredients"},{"location":"3_execution/#idea_1","text":"Given that context, we could compute some \"weights\" for the given ingredients on the database, where the idea is to express in a score with range [0, 1] how \"common\" is an ingredient. Therefore, the recommendations that will select additional ingredients that are not part of the input, could prefer \"common\" ingredients to be recommended instead of \"not common\" ones. So, through an analysis on the data available, ingredients were processed to get frecuency and scale it into a range of [0,1] with a logarithmic scale. The result was saved into a YAML file , and the approach implemented was simply recommend recipes that: - Are prone to contain the input ingredients (with high score) - Are prone to contain \"common\" ingredients (with the explained score) Info Notice that the YAML file indicates which is the list of supported ingredients, and the relevance they have in the given database. Finally, this was the approach selected to go production .","title":"Idea"},{"location":"3_execution/#service-development","text":"Once the recommendation approach was defined and developed, the final step is to develop and deploy a service that receives requests from the StarvApp with input of users and returns recipes recommendations. Therefore, a script was developed with asyncio to do the following steps: Set an asynchronous connection to an existing RabbitMQ queue with aio-pika When a message is received, the service calls the function to process the input ingredients and return the recommended recipes A response is delivered with the recommendations, on the same RabbitMQ queue. For more details about the integration between services in the application, check the architecture shown in Chapter 2 . To test the service before the deployment, another script was developed to emulate requests with a random input to ensure the service will respond correctly. To deploy this service, a Dockerfile was developed to install the system and run the service as entrypoint. At the same time, another Dockerfile was defined to also run the RabbitMQ service for the application. Notice that both services are managed by the DevOps team in their given infrastructure.","title":"Service development"},{"location":"3_execution/#testing","text":"For this solution, there where two types of testing implemented: Unit testing: To validate each module or block of code on the starvapprecom package mostly for code coverage purposes, which are implemented as simple functions executed by pytest . System testing: To validate some functional tests , as checks over the recommender with a dataset regarding some specific scenarios , each of them having corresponding checks . Info To run the system tests, a dataset must be available and with a configured valid path","title":"Testing"},{"location":"3_execution/#deployment","text":"The StarvApp has a Continuous Integration (CI) pipeline configured in terms of Azure pipelines and handled by the DevOps team, so the CI is already managed in this work and the files explained in the Environment section are the contract to ensure correct integration. However, additionally a Github Actions workflow was configured for this repository to make the following checks: Smoke testing based on configured tests Code coverage along the releases Correct linting You can check the configuration and current status in the project's README .","title":"Deployment"},{"location":"4_acceptance/","text":"Chapter 4: Acceptance \u00b6 Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase... Validation \u00b6 Results \u00b6 Next Steps \u00b6","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#chapter-4-acceptance","text":"Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#validation","text":"","title":"Validation"},{"location":"4_acceptance/#results","text":"","title":"Results"},{"location":"4_acceptance/#next-steps","text":"","title":"Next Steps"},{"location":"5_epilogue/","text":"Epilogue \u00b6 Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"5_epilogue/#epilogue","text":"Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"how-to/data_quality_control/","text":"How-To: Data Quality Control \u00b6 Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks. Resources \u00b6 Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional) Setup \u00b6 Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control. Execution \u00b6 1. Set goals and scope \u00b6 It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review. 2. Analyze data \u00b6 Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA). 3. Document results and conclusions \u00b6 Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified. Follow-up \u00b6 The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Data Quality Control"},{"location":"how-to/data_quality_control/#how-to-data-quality-control","text":"Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks.","title":"How-To: Data Quality Control"},{"location":"how-to/data_quality_control/#resources","text":"Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional)","title":"Resources"},{"location":"how-to/data_quality_control/#setup","text":"Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control.","title":"Setup"},{"location":"how-to/data_quality_control/#execution","text":"","title":"Execution"},{"location":"how-to/data_quality_control/#1-set-goals-and-scope","text":"It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review.","title":"1. Set goals and scope"},{"location":"how-to/data_quality_control/#2-analyze-data","text":"Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA).","title":"2. Analyze data"},{"location":"how-to/data_quality_control/#3-document-results-and-conclusions","text":"Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified.","title":"3. Document results and conclusions"},{"location":"how-to/data_quality_control/#follow-up","text":"The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Follow-up"}]}