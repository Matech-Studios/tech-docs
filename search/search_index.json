{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Playbook: Recipes Recommender for StarvApp \u00b6 Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done. Contents \u00b6 These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution. Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. Chapter 2: Preparation : Setup and preparation of the resources and the working environment. Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. Epilogue : Facts occurred after the solution was deployed and validated (optional). How-to Guides \u00b6 These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-04-17 hello@matechstudios.com","title":"Home"},{"location":"#playbook-recipes-recommender-for-starvapp","text":"Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done.","title":"Playbook: Recipes Recommender for StarvApp"},{"location":"#contents","text":"These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution. Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. Chapter 2: Preparation : Setup and preparation of the resources and the working environment. Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. Epilogue : Facts occurred after the solution was deployed and validated (optional).","title":"Contents"},{"location":"#how-to-guides","text":"These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-04-17 hello@matechstudios.com","title":"How-to Guides"},{"location":"0_preface/","text":"Preface \u00b6 Abstract This section introduces the most relevant information of the solution detailed in the playbook. Goal \u00b6 Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database. Scope \u00b6 The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version. Technologies \u00b6 These are the most relevant technologies used during the development. Languages \u00b6 Name Version Description Python 3.8 Main development Bash 5.0 Util scripts Data Processing \u00b6 Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring Messages broker \u00b6 Name Version Description Aio-pika 6.7 Connection to RabbitMQ with asyncio Testing \u00b6 Name Version Description Pytest 6.2 Unit and System testing of service Team \u00b6 Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Preface"},{"location":"0_preface/#preface","text":"Abstract This section introduces the most relevant information of the solution detailed in the playbook.","title":"Preface"},{"location":"0_preface/#goal","text":"Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database.","title":"Goal"},{"location":"0_preface/#scope","text":"The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version.","title":"Scope"},{"location":"0_preface/#technologies","text":"These are the most relevant technologies used during the development.","title":"Technologies"},{"location":"0_preface/#languages","text":"Name Version Description Python 3.8 Main development Bash 5.0 Util scripts","title":"Languages"},{"location":"0_preface/#data-processing","text":"Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring","title":"Data Processing"},{"location":"0_preface/#messages-broker","text":"Name Version Description Aio-pika 6.7 Connection to RabbitMQ with asyncio","title":"Messages broker"},{"location":"0_preface/#testing","text":"Name Version Description Pytest 6.2 Unit and System testing of service","title":"Testing"},{"location":"0_preface/#team","text":"Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Team"},{"location":"1_understanding/","text":"Chapter 1: Understanding \u00b6 Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 8 min read ) Context \u00b6 StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information. Problem \u00b6 Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. For more details about the design of the poll, you can check the How-To: Poll to Collect Requirements document. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project's team if possible, to ensure there are opinions biased by the project flow. Requirements \u00b6 Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Chapter 1: Understanding"},{"location":"1_understanding/#chapter-1-understanding","text":"Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 8 min read )","title":"Chapter 1: Understanding"},{"location":"1_understanding/#context","text":"StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information.","title":"Context"},{"location":"1_understanding/#problem","text":"Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. For more details about the design of the poll, you can check the How-To: Poll to Collect Requirements document. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project's team if possible, to ensure there are opinions biased by the project flow.","title":"Problem"},{"location":"1_understanding/#requirements","text":"Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Requirements"},{"location":"2_preparation/","text":"Chapter 2: Preparation \u00b6 Abstract This chapter covers the preparation in terms of the approach and architecture defined for the solution, as well as the setup of the development environment. ( X min read ) Approach \u00b6 Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it. Assumptions \u00b6 This approach was elaborated taken into account these restrictions and considerations This approach is intended for a \"cold start\" , where no users history is available Therefore, recommendations are obtained only based on the recipes information, not from users Assuming that available recipes are received from input Idea \u00b6 In this \"naive\" approach, the idea is to get the recipes with best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach. Metrics \u00b6 In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 More details about approach in this Jupyter notebook . Architecture \u00b6 Info To be filled soon... Setup \u00b6 Code Repository \u00b6 From scaffold? List Dependencies Info To be filled soon... Environment \u00b6 Info To be filled soon... Continuous Integration \u00b6 Github Actions, and Azure Pipelines Info To be filled soon...","title":"Chapter 2: Preparation"},{"location":"2_preparation/#chapter-2-preparation","text":"Abstract This chapter covers the preparation in terms of the approach and architecture defined for the solution, as well as the setup of the development environment. ( X min read )","title":"Chapter 2: Preparation"},{"location":"2_preparation/#approach","text":"Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it.","title":"Approach"},{"location":"2_preparation/#assumptions","text":"This approach was elaborated taken into account these restrictions and considerations This approach is intended for a \"cold start\" , where no users history is available Therefore, recommendations are obtained only based on the recipes information, not from users Assuming that available recipes are received from input","title":"Assumptions"},{"location":"2_preparation/#idea","text":"In this \"naive\" approach, the idea is to get the recipes with best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach.","title":"Idea"},{"location":"2_preparation/#metrics","text":"In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 More details about approach in this Jupyter notebook .","title":"Metrics"},{"location":"2_preparation/#architecture","text":"Info To be filled soon...","title":"Architecture"},{"location":"2_preparation/#setup","text":"","title":"Setup"},{"location":"2_preparation/#code-repository","text":"From scaffold? List Dependencies Info To be filled soon...","title":"Code Repository"},{"location":"2_preparation/#environment","text":"Info To be filled soon...","title":"Environment"},{"location":"2_preparation/#continuous-integration","text":"Github Actions, and Azure Pipelines Info To be filled soon...","title":"Continuous Integration"},{"location":"3_execution/","text":"Chapter 3: Execution \u00b6 Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( X min read ) Development \u00b6 Data Analysis \u00b6 In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, we must conduct a Data Quality Control. Check the How-to: Data Quality Control document for details. Info To be filled soon... Modeling \u00b6 Scoring recipes based on: - ... - ... Info To be filled soon... Service \u00b6 A script is provided, which runs a consumer connected to RabbitMQ with aio-pika. Using asyncio library to have asynchronous behavior Info To be filled soon... Testing \u00b6 Unit testing for the score functions implemented The integration tests are performed with the backend Info To be filled soon... Deployment \u00b6 DevOps team configured an Azure pipeline.. We provided one Dockerfile for the Analytics service, and one for the RabbitMQ service. Info To be filled soon...","title":"Chapter 3: Execution"},{"location":"3_execution/#chapter-3-execution","text":"Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( X min read )","title":"Chapter 3: Execution"},{"location":"3_execution/#development","text":"","title":"Development"},{"location":"3_execution/#data-analysis","text":"In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, we must conduct a Data Quality Control. Check the How-to: Data Quality Control document for details. Info To be filled soon...","title":"Data Analysis"},{"location":"3_execution/#modeling","text":"Scoring recipes based on: - ... - ... Info To be filled soon...","title":"Modeling"},{"location":"3_execution/#service","text":"A script is provided, which runs a consumer connected to RabbitMQ with aio-pika. Using asyncio library to have asynchronous behavior Info To be filled soon...","title":"Service"},{"location":"3_execution/#testing","text":"Unit testing for the score functions implemented The integration tests are performed with the backend Info To be filled soon...","title":"Testing"},{"location":"3_execution/#deployment","text":"DevOps team configured an Azure pipeline.. We provided one Dockerfile for the Analytics service, and one for the RabbitMQ service. Info To be filled soon...","title":"Deployment"},{"location":"4_acceptance/","text":"Chapter 4: Acceptance \u00b6 Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase... Validation \u00b6 Results \u00b6 Next Steps \u00b6","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#chapter-4-acceptance","text":"Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#validation","text":"","title":"Validation"},{"location":"4_acceptance/#results","text":"","title":"Results"},{"location":"4_acceptance/#next-steps","text":"","title":"Next Steps"},{"location":"5_epilogue/","text":"Epilogue \u00b6 Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"5_epilogue/#epilogue","text":"Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"how-to/data_quality_control/","text":"How-To: Data Quality Control \u00b6 Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks. Resources \u00b6 Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional) Setup \u00b6 Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control. Execution \u00b6 1. Set goals and scope \u00b6 It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review. 2. Analyze data \u00b6 Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA). 3. Document results and conclusions \u00b6 Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified. Follow-up \u00b6 The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Data Quality Control"},{"location":"how-to/data_quality_control/#how-to-data-quality-control","text":"Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks.","title":"How-To: Data Quality Control"},{"location":"how-to/data_quality_control/#resources","text":"Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional)","title":"Resources"},{"location":"how-to/data_quality_control/#setup","text":"Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control.","title":"Setup"},{"location":"how-to/data_quality_control/#execution","text":"","title":"Execution"},{"location":"how-to/data_quality_control/#1-set-goals-and-scope","text":"It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review.","title":"1. Set goals and scope"},{"location":"how-to/data_quality_control/#2-analyze-data","text":"Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA).","title":"2. Analyze data"},{"location":"how-to/data_quality_control/#3-document-results-and-conclusions","text":"Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified.","title":"3. Document results and conclusions"},{"location":"how-to/data_quality_control/#follow-up","text":"The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Follow-up"},{"location":"how-to/poll_requirements/","text":"How-To: Poll to Collect Requirements \u00b6 Abstract This guide is intended to provide some guidelines to conduct a poll in order to collect better requirements from stakeholders with different perspectives about the solution. Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"Poll to Collect Requirements"},{"location":"how-to/poll_requirements/#how-to-poll-to-collect-requirements","text":"Abstract This guide is intended to provide some guidelines to conduct a poll in order to collect better requirements from stakeholders with different perspectives about the solution. Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"How-To: Poll to Collect Requirements"},{"location":"how-to/service_aio-pika/","text":"How-To: Asynchronous Service with aio-pika \u00b6 Abstract This guide is intended to provide some guidelines to ... Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"Asynchronous Service with aio-pika"},{"location":"how-to/service_aio-pika/#how-to-asynchronous-service-with-aio-pika","text":"Abstract This guide is intended to provide some guidelines to ... Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"How-To: Asynchronous Service with aio-pika"}]}