{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Playbook: Recipes Recommender for StarvApp \u00b6 Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done. Contents \u00b6 These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution ( 3 min read ) Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. ( 3 min read ) Chapter 2: Preparation : Setup and preparation of the resources and the working environment. ( 5 min read ) Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. ( 1 min read ) Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. ( 1 min read ) Epilogue : Facts occurred after the solution was deployed and validated (optional). ( 1 min read ) How-to Guides \u00b6 These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-05-02 hello@matechstudios.com","title":"Home"},{"location":"#playbook-recipes-recommender-for-starvapp","text":"Description This playbook is intended to explain how the solution developed in this repository was obtained, from the initial understanding of the problem to the development and validation. It is written as a book with different chapters that cover the details corresponding to different stages of the work done.","title":"Playbook: Recipes Recommender for StarvApp"},{"location":"#contents","text":"These are the sections that compose the content of this playboook. Preface : Introduction with most relevant information about the solution ( 3 min read ) Chapter 1: Understanding : Definitions of the problem and the context involved, and the requirements identified. ( 3 min read ) Chapter 2: Preparation : Setup and preparation of the resources and the working environment. ( 5 min read ) Chapter 3: Execution : Information of the involved development, testing and deployment done for the solution. ( 1 min read ) Chapter 4: Acceptance : Validation and results obtained, as well as next steps identified. ( 1 min read ) Epilogue : Facts occurred after the solution was deployed and validated (optional). ( 1 min read )","title":"Contents"},{"location":"#how-to-guides","text":"These are complementary guides about aspects covered on this playbook. Data Quality Control : Guidelines to ensure a dataset has the expected content and quality. Authors Last revision Contact Leandro Ferrado 2021-05-02 hello@matechstudios.com","title":"How-to Guides"},{"location":"0_preface/","text":"Preface \u00b6 Abstract This section introduces the most relevant information of the solution detailed in the playbook. ( 3 min read ) Background \u00b6 StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. In order to make suitable recommendations, we decided to create a new service that interact with the existing services in the application to provide a selection of recommended recipes for the input ingredients entered by the user. The approach taken to recommend recipes is through a basic matching schema that uses fields like score, matching ingredients, and time preparation to rank the best recipes for a given input. Then, a Python service was developed that connects to a AMQP queue to recipe input and deliver recommendations in real time. This playbook is intented to explain how this recipes recommender was developed, from the understanding of the problem to the validation of the solution integrated in the application. It is written for a reader that is interested in knowing how to obtain a recommender system for this context of the application, so the information will have technical details at different levels. Goal \u00b6 Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database. Scope \u00b6 The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version. Technologies \u00b6 These are the most relevant technologies used during the development. Languages \u00b6 Name Version Description Python 3.8 Main development Bash 5.0 Util scripts Data Processing \u00b6 Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring Messages broker \u00b6 Name Version Description Aio-pika 6.7 Connection to RabbitMQ (AMQP connection) with asyncio capabilities Testing \u00b6 Name Version Description Pytest 6.2 Unit and System testing of service Team \u00b6 Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Preface"},{"location":"0_preface/#preface","text":"Abstract This section introduces the most relevant information of the solution detailed in the playbook. ( 3 min read )","title":"Preface"},{"location":"0_preface/#background","text":"StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. In order to make suitable recommendations, we decided to create a new service that interact with the existing services in the application to provide a selection of recommended recipes for the input ingredients entered by the user. The approach taken to recommend recipes is through a basic matching schema that uses fields like score, matching ingredients, and time preparation to rank the best recipes for a given input. Then, a Python service was developed that connects to a AMQP queue to recipe input and deliver recommendations in real time. This playbook is intented to explain how this recipes recommender was developed, from the understanding of the problem to the validation of the solution integrated in the application. It is written for a reader that is interested in knowing how to obtain a recommender system for this context of the application, so the information will have technical details at different levels.","title":"Background"},{"location":"0_preface/#goal","text":"Develop a service that is able to recommend a top of recipes for the ingredients entered by the user and the set of recipes available in the database.","title":"Goal"},{"location":"0_preface/#scope","text":"The service will receive the ingredients selected by the user, and a connection to the database with the available recipes, and it will return a top selection of recipes based on the attributes of recipes and ingredients. The services will not use the users' behavior to select the recommended recipes in this version.","title":"Scope"},{"location":"0_preface/#technologies","text":"These are the most relevant technologies used during the development.","title":"Technologies"},{"location":"0_preface/#languages","text":"Name Version Description Python 3.8 Main development Bash 5.0 Util scripts","title":"Languages"},{"location":"0_preface/#data-processing","text":"Name Version Description Pandas 1.1 Tabular data manipulation Scikit-learn 0.23 Columns processing for scoring","title":"Data Processing"},{"location":"0_preface/#messages-broker","text":"Name Version Description Aio-pika 6.7 Connection to RabbitMQ (AMQP connection) with asyncio capabilities","title":"Messages broker"},{"location":"0_preface/#testing","text":"Name Version Description Pytest 6.2 Unit and System testing of service","title":"Testing"},{"location":"0_preface/#team","text":"Leandro Ferrado ( @leferrad ) - Data Scientist & Backend developer Implement algorithm to recommend recipes Data quality control of recipes stored in database Develop services for recipes recommender","title":"Team"},{"location":"1_understanding/","text":"Chapter 1: Understanding \u00b6 Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 3 min read ) Context \u00b6 StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information. The stakeholders involved in this development are mainly the Product Owner of the application and the teams that develop the other services: Frontend team, in charge of the UI and the UX of the application. Backend team, in charge of the services of the application (endpoints, database, integrations). Analytics team (which develops this service), in charge of extracting data analytics to enrich the information presented to the user. DevOps team, in charge of the infrastructure where the application is deployed. Finally, the application will be in Spanish language to allow a deployment in Argentina. Therefore, the text information to process will be in that language. Problem \u00b6 Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. For more details about the design of the poll, you can check the How-To: Poll to Collect Requirements document. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project if possible, to ensure there are opinions biased by the project flow. Definitions \u00b6 Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Chapter 1: Understanding"},{"location":"1_understanding/#chapter-1-understanding","text":"Abstract This chapter covers the work done to understand the problem to solve and the context involved, in order to define the requirements for the solution. ( 3 min read )","title":"Chapter 1: Understanding"},{"location":"1_understanding/#context","text":"StarvApp is an application intended to provide suitable recipes for what the user has in the fridge. Therefore, the application should be able to select the best recipes that match with the ingredients that the user has, as well as the cooking habits collected from the usage of the application. The solution described in this playbook is the service responsible for processing the user input (ingredients) and the available information on the App to recommend a top of recipes. At the moment of starting this solution, the StarvApp already has an architecture implemented with an application interacting with backend services, so the agreed scope for this work was to create a new service that helps the existing backend to solve the recipes recommendations. In addition, a database of recipes and ingredients was implemented and checked to ensure the available fields are enough to implement a recommendations schema. No data about users behavior was available, so the scope for the recommendations was focused on matching recipes and ingredients information. The stakeholders involved in this development are mainly the Product Owner of the application and the teams that develop the other services: Frontend team, in charge of the UI and the UX of the application. Backend team, in charge of the services of the application (endpoints, database, integrations). Analytics team (which develops this service), in charge of extracting data analytics to enrich the information presented to the user. DevOps team, in charge of the infrastructure where the application is deployed. Finally, the application will be in Spanish language to allow a deployment in Argentina. Therefore, the text information to process will be in that language.","title":"Context"},{"location":"1_understanding/#problem","text":"Based on the given context of the StarvApp development, we could state that the need was to have a service that could be called by the backend to do the task of matching a top of recipes. In order to understand the expected matching of recipes to have in this service, a poll was conducted with the team. The results can be found in this Confluence page , where we can summarize the following insights: Recommended recipes should require no more than the entered ingredients The following aspects are not prioritized: Ability to refine recipes selection. Avoiding repeated recommendations. Ability to discover new recipes. Preference for few but accurate recommendations rather than selecting best recipes from a list through filters. Recommender should be fast. For more details about the design of the poll, you can check the How-To: Poll to Collect Requirements document. Tip Ensure this poll is filled by stakeholders from different perspectives, even outside the project if possible, to ensure there are opinions biased by the project flow.","title":"Problem"},{"location":"1_understanding/#definitions","text":"Based on the understanding detailed, we defined the following required aspects on the solution to develop: Recommendations of 20 recipes per call. Recipes must match the entered ingredients. Recommendations should use the database of recipes and ingredients. Recipes should not include more than entered ingredients. Deploy a service that communicates with Backend through RabbitMQ Service must provide an output in less than 3 seconds.","title":"Definitions"},{"location":"2_preparation/","text":"Chapter 2: Preparation \u00b6 Abstract This chapter covers the preparation in terms of the approach and architecture defined for the solution, as well as the setup of the development environment. ( 5 min read ) Approach \u00b6 Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it. Assumptions \u00b6 This approach was elaborated taken into account these restrictions and considerations This approach is intended for a \"cold start\" , where no users history is available Therefore, recommendations are obtained only based on the recipes information, not from users Assuming that available recipes are received from input Idea \u00b6 In this \"naive\" approach, the idea is to get the recipes with best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach. Metrics \u00b6 In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 More details about approach in this Jupyter notebook . Architecture \u00b6 The solution must be integrated with the rest of the application by interacting with the backend through a proper bus. Therefore, this is the general architecture that the service had to meet to allow the integration. These are the steps that the application takes in the normal flow: The App registers the input of the user and requests recipes to the Recipes API . The Recipes API gets data from the Database and do the pre-processing of the data to prepare it for the Data Analytics service. The Recipes API publishes the pre-processed data to the Events Bus and waits for the recommended recipes that the Data Analitycs will return by subscribing to a configured topic of the bus (e.g. recipes/{clientId} ). The Data Analitycs is subscribed to the configured topic of the Event Bus and gets the pre-processed data published by the Recipes API . The Data Analitycs processes the data to get a set of recommended recipes, and publishes the selected recipes to the Event Bus on the same configured topic. The Recipes API gets the data with the recommended recipes from the Events Bus . The Recipes API returns the response to the App . It is important to highlight that if this process from step 4 to 6 takes more than a configured window in seconds, the Recipes API will return a default response to the App which is just a header of the recipes retrieved in the step 2. Setup \u00b6 Code Repository \u00b6 At this moment of the project, we didn't have a proper scaffolding nor boilerplate to setup the code repository. Therefore, a new repository was started from scratch, taking into account: Repository name following the name convention followed in Matech, so it was named Matech.Starvapp.RecipesRecomender.Analytics . A Python package to develop the actual logic to process the data A folder with the script for the service A folder to place the notebooks intended for exploration and checks A folder to develop the tests for the solution at unit and system level A folder for development tools (e.g. run linter, set environment, run tests) The repository was published here . You can check how the initial structure was defined in this Pull Request , obtaining this: \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 starvapprecom <- Core package of this project. \u2502 \u251c\u2500\u2500 approaches <- Module to develop approaches for recipes recommendations \u2502 \u251c\u2500\u2500 scrapers <- Module to handle scrapers of recipes \u2502 \u2514\u2500\u2500 utils <- Module to provide utils in general \u2502 \u251c\u2500\u2500 data <- Folder for datasets to be used locally \u2502 \u251c\u2500\u2500 docs <- Project documentation and resources \u2502 \u251c\u2500\u2500 notebooks <- Place to store all Jupyter notebooks \u2502 \u251c\u2500\u2500 scripts <- Scripts to execute services and other functions \u2502 \u251c\u2500\u2500 tests <- Unit and System tests of the core library \u2502 \u251c\u2500\u2500 tools <- Tools for the development of this project \u2502 \u2514\u2500\u2500\u2500 requirements.txt <- File that specifies the dependencies for this project Environment \u00b6 All the services in StarvApp are deployed through Kubernetes , so each of them must provide a Dockerfile that allows the DevOps team to achieve the actual deployment. Therefore, for this repository we provided: A Dockerfile that creates and serves the recommender solution A Dockerfile_rabbitmq that creates a RabbitMQ server for testing purposes A docker-compose.yml to configure the connection between these two previous services, for testing purposes. In addition, to allow an easy way to make local development, there are tools to create and use a virtualenv which is easier to handle for local tests. You can check both configurations in the actual code repository. Continuous Integration \u00b6 The StarvApp has a Continuous Integration (CI) pipeline configured and handled by the DevOps team, so the CI is already managed in this work and the files explained in the Environment section are the contract to ensure correct integration. However, additionally a Github Actions workflow was configured for this repository to make the following checks: Smoke testing based on configured tests Code coverage along the releases Correct linting You can check the configuration and current status in the project's README.","title":"Chapter 2: Preparation"},{"location":"2_preparation/#chapter-2-preparation","text":"Abstract This chapter covers the preparation in terms of the approach and architecture defined for the solution, as well as the setup of the development environment. ( 5 min read )","title":"Chapter 2: Preparation"},{"location":"2_preparation/#approach","text":"Based on the available data, the decision was to start with a simple approach of basic scoring, in order to accelerate integration and collect early feedback to justify the need for a more complex approach Tip Following the KISS principle , it is suggested to always start with a very simple approach (even if it is not Machine Learning), and once it is implemented you can collect feedback and data about usage and then refine it.","title":"Approach"},{"location":"2_preparation/#assumptions","text":"This approach was elaborated taken into account these restrictions and considerations This approach is intended for a \"cold start\" , where no users history is available Therefore, recommendations are obtained only based on the recipes information, not from users Assuming that available recipes are received from input","title":"Assumptions"},{"location":"2_preparation/#idea","text":"In this \"naive\" approach, the idea is to get the recipes with best score based on the following aspects: Matching of ingredients entered Ratings of recipes Cooking time required By tuning the score functions of these aspects, as well as the way to aggregate them, we could have a very quick way to select the best recipes to recommend, in order to be able to start the integration with other services and then have the structure needed to try a better approach.","title":"Idea"},{"location":"2_preparation/#metrics","text":"In order to assess if a set of recommended recipes is appropriate for a set of entered ingredients, these are the aspects to be evaluated: Quality of matched ingredients, by checking average matching ratio of ingredients > 0.5 Popularity of recipes, by checking average rating > 4.0 Complexity of recipes, by checking average cooking time < 60 More details about approach in this Jupyter notebook .","title":"Metrics"},{"location":"2_preparation/#architecture","text":"The solution must be integrated with the rest of the application by interacting with the backend through a proper bus. Therefore, this is the general architecture that the service had to meet to allow the integration. These are the steps that the application takes in the normal flow: The App registers the input of the user and requests recipes to the Recipes API . The Recipes API gets data from the Database and do the pre-processing of the data to prepare it for the Data Analytics service. The Recipes API publishes the pre-processed data to the Events Bus and waits for the recommended recipes that the Data Analitycs will return by subscribing to a configured topic of the bus (e.g. recipes/{clientId} ). The Data Analitycs is subscribed to the configured topic of the Event Bus and gets the pre-processed data published by the Recipes API . The Data Analitycs processes the data to get a set of recommended recipes, and publishes the selected recipes to the Event Bus on the same configured topic. The Recipes API gets the data with the recommended recipes from the Events Bus . The Recipes API returns the response to the App . It is important to highlight that if this process from step 4 to 6 takes more than a configured window in seconds, the Recipes API will return a default response to the App which is just a header of the recipes retrieved in the step 2.","title":"Architecture"},{"location":"2_preparation/#setup","text":"","title":"Setup"},{"location":"2_preparation/#code-repository","text":"At this moment of the project, we didn't have a proper scaffolding nor boilerplate to setup the code repository. Therefore, a new repository was started from scratch, taking into account: Repository name following the name convention followed in Matech, so it was named Matech.Starvapp.RecipesRecomender.Analytics . A Python package to develop the actual logic to process the data A folder with the script for the service A folder to place the notebooks intended for exploration and checks A folder to develop the tests for the solution at unit and system level A folder for development tools (e.g. run linter, set environment, run tests) The repository was published here . You can check how the initial structure was defined in this Pull Request , obtaining this: \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 starvapprecom <- Core package of this project. \u2502 \u251c\u2500\u2500 approaches <- Module to develop approaches for recipes recommendations \u2502 \u251c\u2500\u2500 scrapers <- Module to handle scrapers of recipes \u2502 \u2514\u2500\u2500 utils <- Module to provide utils in general \u2502 \u251c\u2500\u2500 data <- Folder for datasets to be used locally \u2502 \u251c\u2500\u2500 docs <- Project documentation and resources \u2502 \u251c\u2500\u2500 notebooks <- Place to store all Jupyter notebooks \u2502 \u251c\u2500\u2500 scripts <- Scripts to execute services and other functions \u2502 \u251c\u2500\u2500 tests <- Unit and System tests of the core library \u2502 \u251c\u2500\u2500 tools <- Tools for the development of this project \u2502 \u2514\u2500\u2500\u2500 requirements.txt <- File that specifies the dependencies for this project","title":"Code Repository"},{"location":"2_preparation/#environment","text":"All the services in StarvApp are deployed through Kubernetes , so each of them must provide a Dockerfile that allows the DevOps team to achieve the actual deployment. Therefore, for this repository we provided: A Dockerfile that creates and serves the recommender solution A Dockerfile_rabbitmq that creates a RabbitMQ server for testing purposes A docker-compose.yml to configure the connection between these two previous services, for testing purposes. In addition, to allow an easy way to make local development, there are tools to create and use a virtualenv which is easier to handle for local tests. You can check both configurations in the actual code repository.","title":"Environment"},{"location":"2_preparation/#continuous-integration","text":"The StarvApp has a Continuous Integration (CI) pipeline configured and handled by the DevOps team, so the CI is already managed in this work and the files explained in the Environment section are the contract to ensure correct integration. However, additionally a Github Actions workflow was configured for this repository to make the following checks: Smoke testing based on configured tests Code coverage along the releases Correct linting You can check the configuration and current status in the project's README.","title":"Continuous Integration"},{"location":"3_execution/","text":"Chapter 3: Execution \u00b6 Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( X min read ) Development \u00b6 Data Analysis \u00b6 In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, we must conduct a Data Quality Control. Check the How-to: Data Quality Control document for details. Info To be filled soon... Modeling \u00b6 Scoring recipes based on: - ... - ... Info To be filled soon... Service \u00b6 A script is provided, which runs a consumer connected to RabbitMQ with aio-pika. Using asyncio library to have asynchronous behavior Info To be filled soon... Testing \u00b6 Unit testing for the score functions implemented The integration tests are performed with the backend Info To be filled soon... Deployment \u00b6 DevOps team configured an Azure pipeline.. We provided one Dockerfile for the Analytics service, and one for the RabbitMQ service. Info To be filled soon...","title":"Chapter 3: Execution"},{"location":"3_execution/#chapter-3-execution","text":"Abstract This chapter covers the execution of the work based on the previous definitions done, in terms of developing, testing, and validating the solutions based on the agreed acceptance criteria. ( X min read )","title":"Chapter 3: Execution"},{"location":"3_execution/#development","text":"","title":"Development"},{"location":"3_execution/#data-analysis","text":"In order to ensure the database of recipes and ingredients as proper quality to implement the given approach, we must conduct a Data Quality Control. Check the How-to: Data Quality Control document for details. Info To be filled soon...","title":"Data Analysis"},{"location":"3_execution/#modeling","text":"Scoring recipes based on: - ... - ... Info To be filled soon...","title":"Modeling"},{"location":"3_execution/#service","text":"A script is provided, which runs a consumer connected to RabbitMQ with aio-pika. Using asyncio library to have asynchronous behavior Info To be filled soon...","title":"Service"},{"location":"3_execution/#testing","text":"Unit testing for the score functions implemented The integration tests are performed with the backend Info To be filled soon...","title":"Testing"},{"location":"3_execution/#deployment","text":"DevOps team configured an Azure pipeline.. We provided one Dockerfile for the Analytics service, and one for the RabbitMQ service. Info To be filled soon...","title":"Deployment"},{"location":"4_acceptance/","text":"Chapter 4: Acceptance \u00b6 Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase... Validation \u00b6 Results \u00b6 Next Steps \u00b6","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#chapter-4-acceptance","text":"Abstract This chapter covers the validation of the solution based on the defined accepted criteria to ensure it is ready for production. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Chapter 4: Acceptance"},{"location":"4_acceptance/#validation","text":"","title":"Validation"},{"location":"4_acceptance/#results","text":"","title":"Results"},{"location":"4_acceptance/#next-steps","text":"","title":"Next Steps"},{"location":"5_epilogue/","text":"Epilogue \u00b6 Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"5_epilogue/#epilogue","text":"Abstract This chapter is optional, and it is intended to show all the relevant information about facts occurred after the solution was deployed and validated. ( X min read ) Info This section will be filled once the solution passes the validation phase...","title":"Epilogue"},{"location":"how-to/data_quality_control/","text":"How-To: Data Quality Control \u00b6 Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks. Resources \u00b6 Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional) Setup \u00b6 Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control. Execution \u00b6 1. Set goals and scope \u00b6 It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review. 2. Analyze data \u00b6 Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA). 3. Document results and conclusions \u00b6 Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified. Follow-up \u00b6 The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Data Quality Control"},{"location":"how-to/data_quality_control/#how-to-data-quality-control","text":"Abstract This guide is intended to provide some guidelines to make a data quality control over a dataset, to ensure the data has the expected content and quality. Use this guide to: Conduct an efficient data quality control with Python tools in a Jupyter notebook. Enhance previous data quality controls done with this presented guideline. Assumptions: Data to be controlled is tabular. Familiarity with Python tools and Jupyter notebooks.","title":"How-To: Data Quality Control"},{"location":"how-to/data_quality_control/#resources","text":"Access to the dataset/s to be analyzed, stored in disk (e.g. in CSV, Parquet, ORC). Development environment with Python 3.x, having permissions to install new dependencies with pip. Jupyter notebook running, having access to UI. Jupyter notebook template for Data Quality Controls (optional)","title":"Resources"},{"location":"how-to/data_quality_control/#setup","text":"Before beggining the analysis, check that everything works correctly. Place the dataset/s in a convenient folder, where you could also save new datasets generated in the analysis. We suggest to create a folder \"data\" at project level where you can place the input dataset/s and store the new ones. Install all the dependencies required for the analysis. It is recommended to use Pandas if the datasets can entirely fit in memory or there are few aggregations to do (most probable scenario). Otherwise, you can use Spark or Dask for larger data. For visualizations, choose Seaborn for static charts and Plotly for interactive ones. Run Jupyter and access to UI (tipically, http://localhost:8888) Create a new notebook (from a template, if available) to run the analysis. Check that all the dependencies are correctly imported in the notebook. Check a correct loading of the dataset/s to control.","title":"Setup"},{"location":"how-to/data_quality_control/#execution","text":"","title":"Execution"},{"location":"how-to/data_quality_control/#1-set-goals-and-scope","text":"It is important to describe the control to be done, to set expectation about the goals of the analysis and a scope to limit what will be done and what won't be controlled. Document goals and scope of the Data Quality Control to be done, in the first cells of the notebok. The goals must be a summary of the outcome expected. For example, \"Control correctness and coverage of columns in the dataset\" and \"Impute missing information with a proper strategy\". The scope should express what is expected to see in the notebook. For example, \"Control coverage of columns\", \"Check distribution of values in variables\", \"Outliers detection\", etc. Check both points are correctly documented, and are clear for another reader. Tip This documentation should guide you during the analysis to avoid doing unnecessary work, as well as to help a reader to set expectations about the work to review.","title":"1. Set goals and scope"},{"location":"how-to/data_quality_control/#2-analyze-data","text":"Now it is time to analyze the data and make some check to control the content and quality. Here some points to conduct this analysis: Start with a quick inspection through a convenient tool that makes an automated analysis, like Pandas profiling or Sweetviz . This will help to have a first outlook of the content and then decide the focus of the checks to be done by manually Decide the tool depending on the size of the data. For instance, Pandas profiling by default does not handle large dataset, but it could be configured to optimize the processing. Info To be continued soon... Tip Ask for peer review in this analysis, mainly to ensure the control done is correct and it covers the needed aspects. Tip Make focus on controling quality on the content rather than analyzing patterns or extracting insights from the data (for those goals you can conduct an Exploratory Data Analysis or EDA).","title":"2. Analyze data"},{"location":"how-to/data_quality_control/#3-document-results-and-conclusions","text":"Once your analysis it is pretty exhaustive to consider it as done, it is time to document the results. Collect all the insights extracted about the control done to summarize them at the end of the notebook in a section \"Summary\". If possible, remark the treatment that the data need to ensure correct quality for later analytic tasks. Analyze those results obtained to document your conclusions about the quality assesed on data in a section \"Conclusions\" Decide if the data has proper quality to proceed with later analytic task, or if some corrections must be done before. Document \"Next steps\" about work to be done, if already identified.","title":"3. Document results and conclusions"},{"location":"how-to/data_quality_control/#follow-up","text":"The quality control is a never end process, so make a follow up to ensure the data sources to use will have the quality intended for a solution. Make corrective actions based on the conclusions extracted. Adjust code regarding processing of this data based on the indicated treatment. Load issues on the corresponding repositories that use the data.","title":"Follow-up"},{"location":"how-to/poll_requirements/","text":"How-To: Poll to Collect Requirements \u00b6 Abstract This guide is intended to provide some guidelines to conduct a poll in order to collect better requirements from stakeholders with different perspectives about the solution. Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"Poll to Collect Requirements"},{"location":"how-to/poll_requirements/#how-to-poll-to-collect-requirements","text":"Abstract This guide is intended to provide some guidelines to conduct a poll in order to collect better requirements from stakeholders with different perspectives about the solution. Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"How-To: Poll to Collect Requirements"},{"location":"how-to/service_aio-pika/","text":"How-To: Asynchronous Service with aio-pika \u00b6 Abstract This guide is intended to provide some guidelines to ... Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"Asynchronous Service with aio-pika"},{"location":"how-to/service_aio-pika/#how-to-asynchronous-service-with-aio-pika","text":"Abstract This guide is intended to provide some guidelines to ... Use this guide to: ... ... Assumptions: ... ... Info To be filled soon...","title":"How-To: Asynchronous Service with aio-pika"}]}